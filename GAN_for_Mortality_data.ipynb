{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "2aqFULZqEhRf"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from google.colab import files\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "MS110MUhEvUN"
   },
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess Data\n",
    "data = pd.read_csv('/content/Dr Ogunjubohun ML data.csv')\n",
    "\n",
    "numerical_features = data.select_dtypes(include=['number']).columns\n",
    "categorical_features = data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "processed_data = preprocessor.fit_transform(data)\n",
    "X_train, X_test = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "yQiVMI5HExb0"
   },
   "outputs": [],
   "source": [
    "#2. Define GAN Architecture\n",
    "noise_dim = 100\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(noise_dim,)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(processed_data.shape[1], activation='relu'))  # Using relu for output\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(processed_data.shape[1],)))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "AJJ4BOgeFHQr"
   },
   "outputs": [],
   "source": [
    "# 3. Compile and Train GAN\n",
    "# Define optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Define loss function\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)  # Added closing parenthesis\n",
    "\n",
    "\n",
    "# Define training step\n",
    "BATCH_SIZE = 32 # Define your batch size\n",
    "EPOCHS = 50 # Define the number of epochs\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):  # Assuming 'images' is your training data (X_train)\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTGXjVLKFoJ4",
    "outputId": "5c4eab81-5fde-4ae5-e542-808d566b9db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# 4. Generate Synthetic Data\n",
    "num_samples = 3000  # Number of synthetic samples to generate\n",
    "noise = np.random.normal(0, 1, (num_samples, noise_dim))\n",
    "synthetic_data = generator.predict(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B5itrVS_GK0i"
   },
   "outputs": [],
   "source": [
    "# 5. Convert back to original form and units\n",
    "num_features = preprocessor.transformers_[0][2].tolist()\n",
    "cat_features = preprocessor.transformers_[1][2].tolist()\n",
    "\n",
    "# Create pipelines for numerical and categorical features\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Fit the pipelines to the original data\n",
    "num_pipeline.fit(data[num_features])\n",
    "cat_pipeline.fit(data[cat_features])\n",
    "\n",
    "# Get the range of the original numerical features\n",
    "num_range = num_pipeline.named_steps['scaler'].data_range_\n",
    "\n",
    "# Apply inverse transform to the numerical features, considering the range\n",
    "synthetic_data_num = synthetic_data[:, :len(num_features)] * num_range\n",
    "\n",
    "# Apply inverse transform to the categorical features\n",
    "synthetic_data_cat = cat_pipeline.inverse_transform(synthetic_data[:, len(num_features):])\n",
    "\n",
    "# Convert the numerical data to a DataFrame\n",
    "synthetic_df_num = pd.DataFrame(synthetic_data_num, columns=num_features)\n",
    "\n",
    "# Convert the categorical data to a DataFrame\n",
    "synthetic_df_cat = pd.DataFrame(synthetic_data_cat, columns=cat_features)\n",
    "\n",
    "# Concatenate the numerical and categorical DataFrames\n",
    "synthetic_df_original = pd.concat([synthetic_df_num, synthetic_df_cat], axis=1)\n",
    "\n",
    "# Ensure the columns are in the same order as the original data\n",
    "synthetic_df_original = synthetic_df_original[data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_lmGFrryGVHx",
    "outputId": "78267b6d-73ad-4373-e8f5-093bd3c7d9e2"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_154ac233-65bf-4b03-8fbd-a67c80c08f44\", \"synthetic_data_original.csv\", 728500)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Download the Synthetic Data\n",
    "synthetic_df_original.to_csv('synthetic_data_original.csv', index=False)\n",
    "files.download('synthetic_data_original.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
